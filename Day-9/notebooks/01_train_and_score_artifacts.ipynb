{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1949fa5-76f2-4ed5-89c5-7bc935abaf7a",
   "metadata": {},
   "source": [
    "### Imports + paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82f58d4-0691-4724-a0b8-219f594b57a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-1\\data\\warehouse\\day1.duckdb\n",
      "Artifacts: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-9\\artifacts\n",
      "Reports: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-9\\reports\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import joblib\n",
    "\n",
    "project_root = Path.cwd().resolve()\n",
    "while not (project_root / \"Day-1\").exists():\n",
    "    if project_root == project_root.parent:\n",
    "        raise FileNotFoundError(\"Could not find project root containing Day-1.\")\n",
    "    project_root = project_root.parent\n",
    "\n",
    "db_path = project_root / \"Day-1\" / \"data\" / \"warehouse\" / \"day1.duckdb\"\n",
    "\n",
    "day9_dir = project_root / \"Day-9\"\n",
    "art_dir = day9_dir / \"artifacts\"\n",
    "rep_dir = day9_dir / \"reports\"\n",
    "\n",
    "art_dir.mkdir(parents=True, exist_ok=True)\n",
    "rep_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DB:\", db_path)\n",
    "print(\"Artifacts:\", art_dir)\n",
    "print(\"Reports:\", rep_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8348f6fb-5198-44da-85a6-1d5dd2e652cb",
   "metadata": {},
   "source": [
    "### Load the modeling table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995a8f12-0de1-4572-9683-aeed2ef3edc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: (101766, 20)\n",
      "prevalence: 0.11159915885462728\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>person_id</th>\n",
       "      <th>label</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>num_lab_procedures</th>\n",
       "      <th>num_procedures</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>number_outpatient</th>\n",
       "      <th>number_emergency</th>\n",
       "      <th>number_inpatient</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>change</th>\n",
       "      <th>insulin</th>\n",
       "      <th>A1Cresult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2278392</td>\n",
       "      <td>8222157</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[0-10)</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149190</td>\n",
       "      <td>55629189</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[10-20)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Up</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64410</td>\n",
       "      <td>86047875</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Female</td>\n",
       "      <td>[20-30)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500364</td>\n",
       "      <td>82442376</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[30-40)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Up</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16680</td>\n",
       "      <td>42519267</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[40-50)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Steady</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   encounter_id  person_id  label  time_in_hospital  num_lab_procedures  \\\n",
       "0       2278392    8222157      0                 1                  41   \n",
       "1        149190   55629189      0                 3                  59   \n",
       "2         64410   86047875      0                 2                  11   \n",
       "3        500364   82442376      0                 2                  44   \n",
       "4         16680   42519267      0                 1                  51   \n",
       "\n",
       "   num_procedures  num_medications  number_outpatient  number_emergency  \\\n",
       "0               0                1                  0                 0   \n",
       "1               0               18                  0                 0   \n",
       "2               5               13                  2                 0   \n",
       "3               1               16                  0                 0   \n",
       "4               0                8                  0                 0   \n",
       "\n",
       "   number_inpatient             race  gender      age admission_type_id  \\\n",
       "0                 0        Caucasian  Female   [0-10)                 6   \n",
       "1                 0        Caucasian  Female  [10-20)                 1   \n",
       "2                 1  AfricanAmerican  Female  [20-30)                 1   \n",
       "3                 0        Caucasian    Male  [30-40)                 1   \n",
       "4                 0        Caucasian    Male  [40-50)                 1   \n",
       "\n",
       "  discharge_disposition_id admission_source_id diabetesMed change insulin  \\\n",
       "0                       25                   1          No     No      No   \n",
       "1                        1                   7         Yes     Ch      Up   \n",
       "2                        1                   7         Yes     No      No   \n",
       "3                        1                   7         Yes     Ch      Up   \n",
       "4                        1                   7         Yes     Ch  Steady   \n",
       "\n",
       "  A1Cresult  \n",
       "0      None  \n",
       "1      None  \n",
       "2      None  \n",
       "3      None  \n",
       "4      None  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = duckdb.connect(str(db_path))\n",
    "df = con.execute(\"SELECT * FROM gold_diabetes_features_v1\").df()\n",
    "con.close()\n",
    "\n",
    "print(\"df shape:\", df.shape)\n",
    "print(\"prevalence:\", float(df[\"label\"].mean()))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aac9fe-f6ea-4bad-b865-06f7e94b8651",
   "metadata": {},
   "source": [
    "### Define “data contract”: IDs, features, and preprocessing\n",
    "\n",
    "This is important for deployment. We explicitly define what columns are required and how they’re handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "075f5605-b435-40fd-97d8-da0f0a35f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold\n",
    "\n",
    "ID_COLS = [\"encounter_id\", \"person_id\", \"label\"]\n",
    "FEATURE_COLS = [c for c in df.columns if c not in ID_COLS]\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    \"time_in_hospital\", \"num_lab_procedures\", \"num_procedures\", \"num_medications\",\n",
    "    \"number_outpatient\", \"number_emergency\", \"number_inpatient\"\n",
    "]\n",
    "CATEGORICAL_COLS = [c for c in FEATURE_COLS if c not in NUMERIC_COLS]\n",
    "\n",
    "def make_dense_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "PREP = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"impute\", SimpleImputer(strategy=\"median\"))]), NUMERIC_COLS),\n",
    "        (\"cat\", Pipeline([(\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"onehot\", make_dense_ohe())]), CATEGORICAL_COLS),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def make_base_model():\n",
    "    return Pipeline([\n",
    "        (\"prep\", PREP),\n",
    "        (\"clf\", HistGradientBoostingClassifier(\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            max_iter=400,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def clip01(p, eps=1e-15):\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    return np.clip(p, eps, 1-eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f0eec-a6a4-4417-aeb3-cbcde8189e79",
   "metadata": {},
   "source": [
    "### Group split (patient leakage safe) + helper metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a142dd-ab86-42ec-99b8-42b070f5dd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test: 60988 20625 20153\n",
      "Overlap train-valid: 0\n",
      "Overlap train-test: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score, brier_score_loss, log_loss\n",
    "\n",
    "y = df[\"label\"].astype(int)\n",
    "groups = df[\"person_id\"]\n",
    "\n",
    "gss1 = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "idx_trainval, idx_test = next(gss1.split(df, y, groups=groups))\n",
    "df_trainval = df.iloc[idx_trainval].copy()\n",
    "df_test = df.iloc[idx_test].copy()\n",
    "\n",
    "gss2 = GroupShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "idx_train, idx_valid = next(gss2.split(df_trainval, df_trainval[\"label\"].astype(int),\n",
    "                                       groups=df_trainval[\"person_id\"]))\n",
    "df_train = df_trainval.iloc[idx_train].copy()\n",
    "df_valid = df_trainval.iloc[idx_valid].copy()\n",
    "\n",
    "print(\"Train/Valid/Test:\", df_train.shape[0], df_valid.shape[0], df_test.shape[0])\n",
    "print(\"Overlap train-valid:\", len(set(df_train[\"person_id\"]) & set(df_valid[\"person_id\"])))\n",
    "print(\"Overlap train-test:\", len(set(df_train[\"person_id\"]) & set(df_test[\"person_id\"])))\n",
    "\n",
    "X_train, y_train, g_train = df_train[FEATURE_COLS], df_train[\"label\"].astype(int).to_numpy(), df_train[\"person_id\"].to_numpy()\n",
    "X_valid, y_valid = df_valid[FEATURE_COLS], df_valid[\"label\"].astype(int).to_numpy()\n",
    "X_test,  y_test  = df_test[FEATURE_COLS],  df_test[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "def compute_metrics(y_true, p):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    p = np.asarray(p).astype(float)\n",
    "    pc = clip01(p)\n",
    "    return {\n",
    "        \"prevalence\": float(y_true.mean()),\n",
    "        \"mean_p\": float(p.mean()),\n",
    "        \"median_p\": float(np.median(p)),\n",
    "        \"pr_auc\": float(average_precision_score(y_true, p)),\n",
    "        \"roc_auc\": float(roc_auc_score(y_true, p)),\n",
    "        \"brier\": float(brier_score_loss(y_true, p)),\n",
    "        \"logloss\": float(log_loss(y_true, pc, labels=[0,1])),\n",
    "    }\n",
    "\n",
    "def topk(y_true, p, frac):\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    p = np.asarray(p).astype(float)\n",
    "    n = len(y_true)\n",
    "    k = max(1, int(np.floor(frac*n)))\n",
    "    idx = np.argsort(-p)[:k]\n",
    "    return {\n",
    "        \"top_frac\": float(frac),\n",
    "        \"k\": int(k),\n",
    "        \"captured\": int(y_true[idx].sum()),\n",
    "        \"precision_at_k\": float(y_true[idx].mean()),\n",
    "        \"threshold\": float(np.quantile(p, 1-frac)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c5db57-3544-419b-aced-ffa7b56c2e63",
   "metadata": {},
   "source": [
    "### Train base model + build OOF Platt calibrator (the Day 6 method)\n",
    "\n",
    "This is the “deployable” probability model: base_model + platt_calibrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "102fdf99-b20f-4597-bf08-9e7867c24cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5 done. cal size=12198\n",
      "Fold 2/5 done. cal size=12198\n",
      "Fold 3/5 done. cal size=12198\n",
      "Fold 4/5 done. cal size=12197\n",
      "Fold 5/5 done. cal size=12197\n",
      "VALID (calibrated): {'prevalence': 0.11461818181818181, 'mean_p': 0.11327204954074022, 'median_p': 0.09911347204037121, 'pr_auc': 0.22467831773854963, 'roc_auc': 0.6677610282599987, 'brier': 0.09682699948900617, 'logloss': 0.33575903386252465}\n",
      "TEST  (calibrated): {'prevalence': 0.10673348881059892, 'mean_p': 0.11166645743436707, 'median_p': 0.09794703429186898, 'pr_auc': 0.2078325210559714, 'roc_auc': 0.6666140354981995, 'brier': 0.0913482386427387, 'logloss': 0.3211129468363118}\n",
      "TEST {'top_frac': 0.01, 'k': 201, 'captured': 73, 'precision_at_k': 0.36318407960199006, 'threshold': 0.3494286102081742}\n",
      "TEST {'top_frac': 0.05, 'k': 1007, 'captured': 304, 'precision_at_k': 0.3018867924528302, 'threshold': 0.23179108027082457}\n",
      "TEST {'top_frac': 0.1, 'k': 2015, 'captured': 502, 'precision_at_k': 0.2491315136476427, 'threshold': 0.19038341536822131}\n",
      "TEST {'top_frac': 0.2, 'k': 4030, 'captured': 829, 'precision_at_k': 0.20570719602977666, 'threshold': 0.14351083032576595}\n"
     ]
    }
   ],
   "source": [
    "# OOF probabilities on TRAIN for calibrator fitting (GroupKFold by person_id)\n",
    "K = 5\n",
    "gkf = GroupKFold(n_splits=K)\n",
    "p_oof = np.zeros(len(df_train), dtype=float)\n",
    "\n",
    "for fold, (tr_idx, cal_idx) in enumerate(gkf.split(X_train, y_train, groups=g_train), start=1):\n",
    "    m = make_base_model()\n",
    "    m.fit(X_train.iloc[tr_idx], y_train[tr_idx])\n",
    "    p_oof[cal_idx] = m.predict_proba(X_train.iloc[cal_idx])[:, 1]\n",
    "    print(f\"Fold {fold}/{K} done. cal size={len(cal_idx)}\")\n",
    "\n",
    "# Platt scaling on logit(p_oof)\n",
    "p_oof_c = clip01(p_oof)\n",
    "z_oof = np.log(p_oof_c / (1 - p_oof_c)).reshape(-1, 1)\n",
    "\n",
    "platt = LogisticRegression(solver=\"lbfgs\", C=1e6, max_iter=2000)\n",
    "platt.fit(z_oof, y_train)\n",
    "\n",
    "# Fit final base model on full TRAIN\n",
    "base_model = make_base_model()\n",
    "base_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on VALID/TEST, apply platt\n",
    "p_valid_raw = base_model.predict_proba(X_valid)[:, 1]\n",
    "p_test_raw  = base_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "z_valid = np.log(clip01(p_valid_raw) / (1 - clip01(p_valid_raw))).reshape(-1, 1)\n",
    "z_test  = np.log(clip01(p_test_raw)  / (1 - clip01(p_test_raw))).reshape(-1, 1)\n",
    "\n",
    "p_valid_hat = platt.predict_proba(z_valid)[:, 1]\n",
    "p_test_hat  = platt.predict_proba(z_test)[:, 1]\n",
    "\n",
    "print(\"VALID (calibrated):\", compute_metrics(y_valid, p_valid_hat))\n",
    "print(\"TEST  (calibrated):\", compute_metrics(y_test,  p_test_hat))\n",
    "\n",
    "for frac in [0.01, 0.05, 0.10, 0.20]:\n",
    "    print(\"TEST\", topk(y_test, p_test_hat, frac))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5fb16e-dbc9-47b3-907d-af41dda7e4e6",
   "metadata": {},
   "source": [
    "### Save artifacts (this is what makes it “deployable”)\n",
    "\n",
    "We save: model, calibrator, and metadata that a real team expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "605691ac-86e6-48c0-bb3e-03b74eb0736f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-9\\artifacts\\readmit_base_model.joblib\n",
      "Saved platt: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-9\\artifacts\\readmit_platt_calibrator.joblib\n",
      "Saved metadata: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-9\\artifacts\\readmit_metadata.json\n",
      "Saved columns: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-9\\artifacts\\readmit_feature_cols.json\n"
     ]
    }
   ],
   "source": [
    "model_path = art_dir / \"readmit_base_model.joblib\"\n",
    "platt_path = art_dir / \"readmit_platt_calibrator.joblib\"\n",
    "meta_path  = art_dir / \"readmit_metadata.json\"\n",
    "cols_path  = art_dir / \"readmit_feature_cols.json\"\n",
    "\n",
    "joblib.dump(base_model, model_path)\n",
    "joblib.dump(platt, platt_path)\n",
    "\n",
    "metadata = {\n",
    "    \"project\": \"diabetes_readmission\",\n",
    "    \"label\": \"readmitted <30 (y_readmit_30)\",\n",
    "    \"split\": \"GroupShuffleSplit by person_id (train/valid/test), calibrator fit on OOF predictions within train (GroupKFold K=5)\",\n",
    "    \"model\": \"HistGradientBoostingClassifier(max_depth=6, learning_rate=0.05, max_iter=400)\",\n",
    "    \"calibration\": \"Platt scaling (logistic regression on logit(p_raw))\",\n",
    "    \"valid_metrics\": compute_metrics(y_valid, p_valid_hat),\n",
    "    \"test_metrics\": compute_metrics(y_test, p_test_hat),\n",
    "    \"topk_test\": [topk(y_test, p_test_hat, f) for f in [0.01, 0.05, 0.10, 0.20]],\n",
    "    \"created_by\": \"Day 9 pipeline\",\n",
    "}\n",
    "\n",
    "meta_path.write_text(json.dumps(metadata, indent=2), encoding=\"utf-8\")\n",
    "cols_path.write_text(json.dumps({\n",
    "    \"id_cols\": ID_COLS,\n",
    "    \"feature_cols\": FEATURE_COLS,\n",
    "    \"numeric_cols\": NUMERIC_COLS,\n",
    "    \"categorical_cols\": CATEGORICAL_COLS\n",
    "}, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved model:\", model_path)\n",
    "print(\"Saved platt:\", platt_path)\n",
    "print(\"Saved metadata:\", meta_path)\n",
    "print(\"Saved columns:\", cols_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf7c8ca-927a-45c6-af3a-08180be1109e",
   "metadata": {},
   "source": [
    "### “Cold start” test: reload artifacts and score again\n",
    "\n",
    "This is a professional sanity check: if this passes, tomorrow’s deployment step becomes straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7f9b8a9-beaa-442e-9b3f-6ac123b169bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded TEST metrics: {'prevalence': 0.10673348881059892, 'mean_p': 0.11166645743436707, 'median_p': 0.09794703429186898, 'pr_auc': 0.2078325210559714, 'roc_auc': 0.6666140354981995, 'brier': 0.0913482386427387, 'logloss': 0.3211129468363118}\n"
     ]
    }
   ],
   "source": [
    "base_model2 = joblib.load(model_path)\n",
    "platt2 = joblib.load(platt_path)\n",
    "\n",
    "p_test_raw2 = base_model2.predict_proba(X_test)[:, 1]\n",
    "z_test2 = np.log(clip01(p_test_raw2) / (1 - clip01(p_test_raw2))).reshape(-1, 1)\n",
    "p_test_hat2 = platt2.predict_proba(z_test2)[:, 1]\n",
    "\n",
    "print(\"Reloaded TEST metrics:\", compute_metrics(y_test, p_test_hat2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7449e4d-3783-4d4d-8f81-fe036fb3ab94",
   "metadata": {},
   "source": [
    "### Create a scored test table artifact (for BI / ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c99e954f-3471-479e-8cc3-39752a09e75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-9\\reports\\DAY09_scored_test.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>person_id</th>\n",
       "      <th>label</th>\n",
       "      <th>p_raw</th>\n",
       "      <th>p_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67387</th>\n",
       "      <td>189144708</td>\n",
       "      <td>42941232</td>\n",
       "      <td>1</td>\n",
       "      <td>0.610624</td>\n",
       "      <td>0.612003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38644</th>\n",
       "      <td>120136542</td>\n",
       "      <td>23838849</td>\n",
       "      <td>1</td>\n",
       "      <td>0.556518</td>\n",
       "      <td>0.557934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87183</th>\n",
       "      <td>277879686</td>\n",
       "      <td>88227540</td>\n",
       "      <td>1</td>\n",
       "      <td>0.542616</td>\n",
       "      <td>0.544036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67883</th>\n",
       "      <td>190944528</td>\n",
       "      <td>57751650</td>\n",
       "      <td>0</td>\n",
       "      <td>0.517605</td>\n",
       "      <td>0.519027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42842</th>\n",
       "      <td>132138702</td>\n",
       "      <td>76743099</td>\n",
       "      <td>0</td>\n",
       "      <td>0.511674</td>\n",
       "      <td>0.513096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       encounter_id  person_id  label     p_raw     p_hat\n",
       "67387     189144708   42941232      1  0.610624  0.612003\n",
       "38644     120136542   23838849      1  0.556518  0.557934\n",
       "87183     277879686   88227540      1  0.542616  0.544036\n",
       "67883     190944528   57751650      0  0.517605  0.519027\n",
       "42842     132138702   76743099      0  0.511674  0.513096"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored_test = df_test[[\"encounter_id\", \"person_id\", \"label\"]].copy()\n",
    "scored_test[\"p_raw\"] = p_test_raw\n",
    "scored_test[\"p_hat\"] = p_test_hat\n",
    "scored_test = scored_test.sort_values(\"p_hat\", ascending=False)\n",
    "\n",
    "scored_path = rep_dir / \"DAY09_scored_test.csv\"\n",
    "scored_test.to_csv(scored_path, index=False)\n",
    "\n",
    "print(\"Saved:\", scored_path)\n",
    "scored_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16280a56-1962-4cbb-836e-c12c98172cb3",
   "metadata": {},
   "source": [
    "### Write a Day 9 model card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69c9dacb-3e3f-407a-a988-831a3f8fa6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-9\\reports\\DAY09_model_card.md\n"
     ]
    }
   ],
   "source": [
    "card = []\n",
    "card.append(\"# Day 9 — Readmission model artifacts (deployable locally)\\n\")\n",
    "card.append(\"## What exists after Day 9\\n\")\n",
    "card.append(\"- Trained base model + Platt calibrator saved as joblib artifacts\\n\")\n",
    "card.append(\"- Metadata JSON with metrics and procedure\\n\")\n",
    "card.append(\"- Feature/column contract JSON\\n\")\n",
    "card.append(\"- Example scored test CSV\\n\")\n",
    "card.append(\"\\n## Test metrics (calibrated)\\n\")\n",
    "tm = metadata[\"test_metrics\"]\n",
    "card.append(f\"- Prevalence: {tm['prevalence']:.6f}\\n\")\n",
    "card.append(f\"- PR-AUC: {tm['pr_auc']:.6f}\\n\")\n",
    "card.append(f\"- ROC-AUC: {tm['roc_auc']:.6f}\\n\")\n",
    "card.append(f\"- Brier: {tm['brier']:.6f}\\n\")\n",
    "card.append(f\"- Log loss: {tm['logloss']:.6f}\\n\")\n",
    "card.append(\"\\n## Capacity targeting (TEST)\\n\")\n",
    "for row in metadata[\"topk_test\"]:\n",
    "    card.append(f\"- Top {int(row['top_frac']*100)}%: k={row['k']} captured={row['captured']} \"\n",
    "                f\"precision@k={row['precision_at_k']:.6f} threshold={row['threshold']:.6f}\\n\")\n",
    "\n",
    "(md_path := rep_dir / \"DAY09_model_card.md\").write_text(\"\".join(card), encoding=\"utf-8\")\n",
    "print(\"Saved:\", md_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
