{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2576b42-d77b-4616-b7e7-3322e7848606",
   "metadata": {},
   "source": [
    "### Connect to DuckDB (same as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e4e2eeb-6ebd-4d41-a40f-346fa8b5bbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-1\\data\\warehouse\\day1.duckdb\n",
      "Tables: ['bronze_diabetes', 'gold_diabetes_base', 'gold_diabetes_features_v1', 'silver_diabetes', 'silver_diabetes_typed']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import duckdb\n",
    "\n",
    "project_root = Path.cwd().resolve()\n",
    "while not (project_root / \"Day-1\").exists():\n",
    "    if project_root == project_root.parent:\n",
    "        raise FileNotFoundError(\"Could not find project root containing Day-1.\")\n",
    "    project_root = project_root.parent\n",
    "\n",
    "db_path = project_root / \"Day-1\" / \"data\" / \"warehouse\" / \"day1.duckdb\"\n",
    "con = duckdb.connect(str(db_path))\n",
    "print(\"Connected to:\", db_path)\n",
    "print(\"Tables:\", [t[0] for t in con.execute(\"SHOW TABLES\").fetchall()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e031d26f-5d16-4c72-9cc3-346a28098abb",
   "metadata": {},
   "source": [
    "### Load the modeling table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036c0f26-6950-4bca-828b-bdf9136bfb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101766, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>person_id</th>\n",
       "      <th>label</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>num_lab_procedures</th>\n",
       "      <th>num_procedures</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>number_outpatient</th>\n",
       "      <th>number_emergency</th>\n",
       "      <th>number_inpatient</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>change</th>\n",
       "      <th>insulin</th>\n",
       "      <th>A1Cresult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2278392</td>\n",
       "      <td>8222157</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[0-10)</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149190</td>\n",
       "      <td>55629189</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[10-20)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Up</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64410</td>\n",
       "      <td>86047875</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Female</td>\n",
       "      <td>[20-30)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500364</td>\n",
       "      <td>82442376</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[30-40)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Up</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16680</td>\n",
       "      <td>42519267</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[40-50)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Steady</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   encounter_id  person_id  label  time_in_hospital  num_lab_procedures  \\\n",
       "0       2278392    8222157      0                 1                  41   \n",
       "1        149190   55629189      0                 3                  59   \n",
       "2         64410   86047875      0                 2                  11   \n",
       "3        500364   82442376      0                 2                  44   \n",
       "4         16680   42519267      0                 1                  51   \n",
       "\n",
       "   num_procedures  num_medications  number_outpatient  number_emergency  \\\n",
       "0               0                1                  0                 0   \n",
       "1               0               18                  0                 0   \n",
       "2               5               13                  2                 0   \n",
       "3               1               16                  0                 0   \n",
       "4               0                8                  0                 0   \n",
       "\n",
       "   number_inpatient             race  gender      age admission_type_id  \\\n",
       "0                 0        Caucasian  Female   [0-10)                 6   \n",
       "1                 0        Caucasian  Female  [10-20)                 1   \n",
       "2                 1  AfricanAmerican  Female  [20-30)                 1   \n",
       "3                 0        Caucasian    Male  [30-40)                 1   \n",
       "4                 0        Caucasian    Male  [40-50)                 1   \n",
       "\n",
       "  discharge_disposition_id admission_source_id diabetesMed change insulin  \\\n",
       "0                       25                   1          No     No      No   \n",
       "1                        1                   7         Yes     Ch      Up   \n",
       "2                        1                   7         Yes     No      No   \n",
       "3                        1                   7         Yes     Ch      Up   \n",
       "4                        1                   7         Yes     Ch  Steady   \n",
       "\n",
       "  A1Cresult  \n",
       "0      None  \n",
       "1      None  \n",
       "2      None  \n",
       "3      None  \n",
       "4      None  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = con.execute(\"SELECT * FROM gold_diabetes_features_v1\").df()\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a13a2b5-ed6a-4fc9-89e2-3b1e7243e07d",
   "metadata": {},
   "source": [
    "### Train/validation/test split (leakage-safe by patient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e43f66-0499-4f75-b56e-3d37fe31760b",
   "metadata": {},
   "source": [
    "We’ll do: test = 20%, then split the remaining into train/valid so overall is roughly 60/20/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf18677-9497-46dd-aba6-21cc5146be2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 60988 Valid rows: 20625 Test rows: 20153\n",
      "Prevalence train/valid/test: 0.11218600380402702 0.11461818181818181 0.10673348881059892\n",
      "Overlap train-valid: 0\n",
      "Overlap train-test: 0\n",
      "Overlap valid-test: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "y = df[\"label\"].astype(int)\n",
    "groups = df[\"person_id\"]\n",
    "\n",
    "# 1) Hold out TEST (20%)\n",
    "gss1 = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "idx_trainval, idx_test = next(gss1.split(df, y, groups=groups))\n",
    "\n",
    "df_trainval = df.iloc[idx_trainval].copy()\n",
    "df_test = df.iloc[idx_test].copy()\n",
    "\n",
    "# 2) Split TRAIN vs VALID inside trainval (valid is 25% of trainval -> 20% of total)\n",
    "y_tv = df_trainval[\"label\"].astype(int)\n",
    "g_tv = df_trainval[\"person_id\"]\n",
    "\n",
    "gss2 = GroupShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "idx_train, idx_valid = next(gss2.split(df_trainval, y_tv, groups=g_tv))\n",
    "\n",
    "df_train = df_trainval.iloc[idx_train].copy()\n",
    "df_valid = df_trainval.iloc[idx_valid].copy()\n",
    "\n",
    "print(\"Train rows:\", df_train.shape[0], \"Valid rows:\", df_valid.shape[0], \"Test rows:\", df_test.shape[0])\n",
    "print(\"Prevalence train/valid/test:\",\n",
    "      float(df_train[\"label\"].mean()),\n",
    "      float(df_valid[\"label\"].mean()),\n",
    "      float(df_test[\"label\"].mean()))\n",
    "\n",
    "# Leakage checks\n",
    "overlap_tv = set(df_train[\"person_id\"]).intersection(set(df_valid[\"person_id\"]))\n",
    "overlap_tt = set(df_train[\"person_id\"]).intersection(set(df_test[\"person_id\"]))\n",
    "overlap_vt = set(df_valid[\"person_id\"]).intersection(set(df_test[\"person_id\"]))\n",
    "print(\"Overlap train-valid:\", len(overlap_tv))\n",
    "print(\"Overlap train-test:\", len(overlap_tt))\n",
    "print(\"Overlap valid-test:\", len(overlap_vt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e5722f-57db-488e-b892-abe697e26d8b",
   "metadata": {},
   "source": [
    "### Define features + preprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5412fe-34eb-4c6f-a0b0-1f10422902ab",
   "metadata": {},
   "source": [
    "We’ll use one preprocessor for linear models (includes scaling) and one for trees (no scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51489c10-0994-42bd-8602-2a411f3f7b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "id_cols = [\"encounter_id\", \"person_id\", \"label\"]\n",
    "feature_cols = [c for c in df.columns if c not in id_cols]\n",
    "\n",
    "numeric_cols = [\n",
    "    \"time_in_hospital\", \"num_lab_procedures\", \"num_procedures\", \"num_medications\",\n",
    "    \"number_outpatient\", \"number_emergency\", \"number_inpatient\"\n",
    "]\n",
    "categorical_cols = [c for c in feature_cols if c not in numeric_cols]\n",
    "\n",
    "X_train = df_train[feature_cols]\n",
    "y_train = df_train[\"label\"].astype(int)\n",
    "\n",
    "X_valid = df_valid[feature_cols]\n",
    "y_valid = df_valid[\"label\"].astype(int)\n",
    "\n",
    "X_test = df_test[feature_cols]\n",
    "y_test = df_test[\"label\"].astype(int)\n",
    "\n",
    "# Linear preprocessor: impute + scale numeric; impute + one-hot categorical\n",
    "prep_linear = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "                          (\"scale\", StandardScaler())]), numeric_cols),\n",
    "        (\"cat\", Pipeline([(\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]), categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Tree preprocessor: impute numeric (no scaling); impute + one-hot categorical\n",
    "prep_tree = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"impute\", SimpleImputer(strategy=\"median\"))]), numeric_cols),\n",
    "        (\"cat\", Pipeline([(\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]), categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ---- Day 4 fix: HistGradientBoosting needs DENSE (not sparse) input ----\n",
    "def make_dense_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)  # newer sklearn\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)         # older sklearn\n",
    "\n",
    "prep_tree_dense = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"impute\", SimpleImputer(strategy=\"median\"))]), numeric_cols),\n",
    "        (\"cat\", Pipeline([(\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"onehot\", make_dense_ohe())]), categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3051e992-26f5-4580-9255-107b1ead17c9",
   "metadata": {},
   "source": [
    "### Define models (baseline + tree models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb346bd4-30ae-4677-9b02-af4b9dd0999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "models = {\n",
    "    \"logreg_unweighted\": Pipeline([\n",
    "        (\"prep\", prep_linear),\n",
    "        (\"clf\", LogisticRegression(max_iter=2000, solver=\"lbfgs\"))\n",
    "    ]),\n",
    "    \"decision_tree\": Pipeline([\n",
    "        (\"prep\", prep_tree),\n",
    "        (\"clf\", DecisionTreeClassifier(\n",
    "            max_depth=6,\n",
    "            min_samples_leaf=200,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ]),\n",
    "    \"random_forest\": Pipeline([\n",
    "        (\"prep\", prep_tree),\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            n_estimators=400,\n",
    "            max_depth=None,\n",
    "            min_samples_leaf=50,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ]),\n",
    "    \"hist_gb\": Pipeline([\n",
    "        (\"prep\", prep_tree_dense),   # <-- this is the fix\n",
    "        (\"clf\", HistGradientBoostingClassifier(\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            max_iter=400,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c161f7-ec7a-42b6-bda6-9c22b5a81a87",
   "metadata": {},
   "source": [
    "### Evaluation functions (PR-AUC, ROC-AUC, Brier, top-K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6cb2dc6-420d-4a4c-a30e-31f19335e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, brier_score_loss\n",
    "\n",
    "def eval_probs(y_true, p):\n",
    "    y_true = np.asarray(y_true)\n",
    "    return {\n",
    "        \"prevalence\": float(y_true.mean()),\n",
    "        \"mean_p\": float(np.mean(p)),\n",
    "        \"pr_auc\": float(average_precision_score(y_true, p)),\n",
    "        \"roc_auc\": float(roc_auc_score(y_true, p)),\n",
    "        \"brier\": float(brier_score_loss(y_true, p)),\n",
    "    }\n",
    "\n",
    "def topk(y_true, p, fracs=(0.01, 0.05, 0.10, 0.20)):\n",
    "    y_true = np.asarray(y_true)\n",
    "    n = len(y_true)\n",
    "    order = np.argsort(-p)\n",
    "    rows = []\n",
    "    for frac in fracs:\n",
    "        k = max(1, int(np.floor(frac*n)))\n",
    "        idx = order[:k]\n",
    "        rows.append({\n",
    "            \"top_frac\": frac,\n",
    "            \"k\": k,\n",
    "            \"captured\": int(np.sum(y_true[idx])),\n",
    "            \"precision_at_k\": float(np.mean(y_true[idx])),\n",
    "            \"threshold\": float(np.quantile(p, 1-frac))\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b077c230-8e9a-4df4-aecc-462605cad82d",
   "metadata": {},
   "source": [
    "### Train on TRAIN, select on VALID, report on TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb567ac5-ec4d-4753-8cdc-2959b63974fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarfo\\AppData\\Local\\Temp\\ipykernel_4276\\2072828327.py:19: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  \"valid_top10_precision\": float(tk_valid.loc[tk_valid[\"top_frac\"]==0.10, \"precision_at_k\"]),\n",
      "C:\\Users\\sarfo\\AppData\\Local\\Temp\\ipykernel_4276\\2072828327.py:22: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  \"test_top10_precision\": float(tk_test.loc[tk_test[\"top_frac\"]==0.10, \"precision_at_k\"]),\n",
      "C:\\Users\\sarfo\\AppData\\Local\\Temp\\ipykernel_4276\\2072828327.py:19: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  \"valid_top10_precision\": float(tk_valid.loc[tk_valid[\"top_frac\"]==0.10, \"precision_at_k\"]),\n",
      "C:\\Users\\sarfo\\AppData\\Local\\Temp\\ipykernel_4276\\2072828327.py:22: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  \"test_top10_precision\": float(tk_test.loc[tk_test[\"top_frac\"]==0.10, \"precision_at_k\"]),\n",
      "C:\\Users\\sarfo\\AppData\\Local\\Temp\\ipykernel_4276\\2072828327.py:19: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  \"valid_top10_precision\": float(tk_valid.loc[tk_valid[\"top_frac\"]==0.10, \"precision_at_k\"]),\n",
      "C:\\Users\\sarfo\\AppData\\Local\\Temp\\ipykernel_4276\\2072828327.py:22: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  \"test_top10_precision\": float(tk_test.loc[tk_test[\"top_frac\"]==0.10, \"precision_at_k\"]),\n",
      "C:\\Users\\sarfo\\AppData\\Local\\Temp\\ipykernel_4276\\2072828327.py:19: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  \"valid_top10_precision\": float(tk_valid.loc[tk_valid[\"top_frac\"]==0.10, \"precision_at_k\"]),\n",
      "C:\\Users\\sarfo\\AppData\\Local\\Temp\\ipykernel_4276\\2072828327.py:22: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  \"test_top10_precision\": float(tk_test.loc[tk_test[\"top_frac\"]==0.10, \"precision_at_k\"]),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>valid_pr_auc</th>\n",
       "      <th>valid_brier</th>\n",
       "      <th>valid_top10_precision</th>\n",
       "      <th>test_pr_auc</th>\n",
       "      <th>test_brier</th>\n",
       "      <th>test_top10_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hist_gb</td>\n",
       "      <td>0.224678</td>\n",
       "      <td>0.096830</td>\n",
       "      <td>0.259942</td>\n",
       "      <td>0.207833</td>\n",
       "      <td>0.091344</td>\n",
       "      <td>0.249132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.224002</td>\n",
       "      <td>0.097494</td>\n",
       "      <td>0.251697</td>\n",
       "      <td>0.204242</td>\n",
       "      <td>0.091860</td>\n",
       "      <td>0.247643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logreg_unweighted</td>\n",
       "      <td>0.222778</td>\n",
       "      <td>0.097112</td>\n",
       "      <td>0.257032</td>\n",
       "      <td>0.200228</td>\n",
       "      <td>0.091835</td>\n",
       "      <td>0.254591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.197429</td>\n",
       "      <td>0.097766</td>\n",
       "      <td>0.245393</td>\n",
       "      <td>0.181440</td>\n",
       "      <td>0.092266</td>\n",
       "      <td>0.241191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model  valid_pr_auc  valid_brier  valid_top10_precision  \\\n",
       "3            hist_gb      0.224678     0.096830               0.259942   \n",
       "2      random_forest      0.224002     0.097494               0.251697   \n",
       "0  logreg_unweighted      0.222778     0.097112               0.257032   \n",
       "1      decision_tree      0.197429     0.097766               0.245393   \n",
       "\n",
       "   test_pr_auc  test_brier  test_top10_precision  \n",
       "3     0.207833    0.091344              0.249132  \n",
       "2     0.204242    0.091860              0.247643  \n",
       "0     0.200228    0.091835              0.254591  \n",
       "1     0.181440    0.092266              0.241191  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leader = []\n",
    "\n",
    "for name, pipe in models.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    p_valid = pipe.predict_proba(X_valid)[:, 1]\n",
    "    p_test  = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    m_valid = eval_probs(y_valid, p_valid)\n",
    "    m_test  = eval_probs(y_test, p_test)\n",
    "\n",
    "    tk_valid = topk(y_valid, p_valid)\n",
    "    tk_test  = topk(y_test, p_test)\n",
    "\n",
    "    leader.append({\n",
    "        \"model\": name,\n",
    "        \"valid_pr_auc\": m_valid[\"pr_auc\"],\n",
    "        \"valid_brier\": m_valid[\"brier\"],\n",
    "        \"valid_top10_precision\": float(tk_valid.loc[tk_valid[\"top_frac\"]==0.10, \"precision_at_k\"]),\n",
    "        \"test_pr_auc\": m_test[\"pr_auc\"],\n",
    "        \"test_brier\": m_test[\"brier\"],\n",
    "        \"test_top10_precision\": float(tk_test.loc[tk_test[\"top_frac\"]==0.10, \"precision_at_k\"]),\n",
    "    })\n",
    "\n",
    "leaderboard = pd.DataFrame(leader).sort_values(\"valid_pr_auc\", ascending=False)\n",
    "leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b508d52-8f32-4604-8c3a-6b5d5a8cad4d",
   "metadata": {},
   "source": [
    "### Save Day-4 artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c1c93ae-630d-46cd-aeff-8a700a618ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-4\\reports\\DAY04_leaderboard.csv\n",
      "Saved: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-4\\reports\\DAY04_leaderboard.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "reports_dir = project_root / \"Day-4\" / \"reports\"\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "leaderboard.to_csv(reports_dir / \"DAY04_leaderboard.csv\", index=False)\n",
    "\n",
    "with open(reports_dir / \"DAY04_leaderboard.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(leader, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", reports_dir / \"DAY04_leaderboard.csv\")\n",
    "print(\"Saved:\", reports_dir / \"DAY04_leaderboard.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e816567-5c6c-4b30-be44-ca0a0818c298",
   "metadata": {},
   "source": [
    "### Save winner model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1694a4-6448-4eea-90e0-5c1c9397f97e",
   "metadata": {},
   "source": [
    "This lets us reload the best model later without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a26da29-96c5-493b-bd7b-04207c808ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: hist_gb\n",
      "Saved: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-4\\reports\\DAY04_best_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "best_name = leaderboard.iloc[0][\"model\"]\n",
    "best_model = models[best_name]\n",
    "best_model.fit(X_train, y_train)   # ensure it's fit\n",
    "\n",
    "joblib.dump(best_model, reports_dir / \"DAY04_best_model.joblib\")\n",
    "print(\"Best model:\", best_name)\n",
    "print(\"Saved:\", reports_dir / \"DAY04_best_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d53b54-ae18-46d5-b866-be7ee36af50f",
   "metadata": {},
   "source": [
    "### Make and save top-200 predictions on the TEST set (winner model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1db5d413-85fd-4e68-b6f9-10121855081f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-4\\reports\\DAY04_top200_test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "p_test_best = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "top = df_test[[\"encounter_id\", \"person_id\", \"label\"]].copy()\n",
    "top[\"p_hat\"] = p_test_best\n",
    "top = top.sort_values(\"p_hat\", ascending=False).head(200)\n",
    "\n",
    "top.to_csv(reports_dir / \"DAY04_top200_test_predictions.csv\", index=False)\n",
    "print(\"Saved:\", reports_dir / \"DAY04_top200_test_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af74c57-5404-4158-808b-fe1fa361eeff",
   "metadata": {},
   "source": [
    "Close DB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8c6f1c3-2766-4e66-9dd5-7f299010e2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB closed.\n"
     ]
    }
   ],
   "source": [
    "con.close()\n",
    "print(\"DuckDB closed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
