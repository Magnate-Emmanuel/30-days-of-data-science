{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad34da3-23ea-4944-88f1-deadf7ef6597",
   "metadata": {},
   "source": [
    "### Connect to DuckDB + load table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c4a843-5acd-4736-8b6d-d181d7d2d2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-1\\data\\warehouse\\day1.duckdb\n",
      "df shape: (101766, 20)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "project_root = Path.cwd().resolve()\n",
    "while not (project_root / \"Day-1\").exists():\n",
    "    if project_root == project_root.parent:\n",
    "        raise FileNotFoundError(\"Could not find project root containing Day-1.\")\n",
    "    project_root = project_root.parent\n",
    "\n",
    "db_path = project_root / \"Day-1\" / \"data\" / \"warehouse\" / \"day1.duckdb\"\n",
    "con = duckdb.connect(str(db_path))\n",
    "print(\"Connected to:\", db_path)\n",
    "\n",
    "df = con.execute(\"SELECT * FROM gold_diabetes_features_v1\").df()\n",
    "print(\"df shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd84de-f815-495c-907d-fa603f35005e",
   "metadata": {},
   "source": [
    "### Leakage-safe train/valid/test split (same as Day 4/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "787bb5ec-f7ed-41ef-8a96-933be1f71f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test: 60988 20625 20153\n",
      "Prevalence train/valid/test: 0.11218600380402702 0.11461818181818181 0.10673348881059892\n",
      "Overlap train-valid: 0\n",
      "Overlap train-test: 0\n",
      "Overlap valid-test: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "y = df[\"label\"].astype(int)\n",
    "groups = df[\"person_id\"]\n",
    "\n",
    "gss1 = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "idx_trainval, idx_test = next(gss1.split(df, y, groups=groups))\n",
    "\n",
    "df_trainval = df.iloc[idx_trainval].copy()\n",
    "df_test = df.iloc[idx_test].copy()\n",
    "\n",
    "gss2 = GroupShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "idx_train, idx_valid = next(gss2.split(df_trainval, df_trainval[\"label\"].astype(int),\n",
    "                                       groups=df_trainval[\"person_id\"]))\n",
    "\n",
    "df_train = df_trainval.iloc[idx_train].copy()\n",
    "df_valid = df_trainval.iloc[idx_valid].copy()\n",
    "\n",
    "print(\"Train/Valid/Test:\", df_train.shape[0], df_valid.shape[0], df_test.shape[0])\n",
    "print(\"Prevalence train/valid/test:\",\n",
    "      float(df_train[\"label\"].mean()),\n",
    "      float(df_valid[\"label\"].mean()),\n",
    "      float(df_test[\"label\"].mean()))\n",
    "\n",
    "print(\"Overlap train-valid:\", len(set(df_train[\"person_id\"]) & set(df_valid[\"person_id\"])))\n",
    "print(\"Overlap train-test:\", len(set(df_train[\"person_id\"]) & set(df_test[\"person_id\"])))\n",
    "print(\"Overlap valid-test:\", len(set(df_valid[\"person_id\"]) & set(df_test[\"person_id\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9118455e-2706-46a4-983d-44f14afc5924",
   "metadata": {},
   "source": [
    "### Preprocessing (dense one-hot) + helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fecce868-91be-4981-b29e-cfd22f0a7262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "id_cols = [\"encounter_id\", \"person_id\", \"label\"]\n",
    "feature_cols = [c for c in df.columns if c not in id_cols]\n",
    "\n",
    "numeric_cols = [\n",
    "    \"time_in_hospital\", \"num_lab_procedures\", \"num_procedures\", \"num_medications\",\n",
    "    \"number_outpatient\", \"number_emergency\", \"number_inpatient\"\n",
    "]\n",
    "categorical_cols = [c for c in feature_cols if c not in numeric_cols]\n",
    "\n",
    "X_train = df_train[feature_cols]\n",
    "y_train = df_train[\"label\"].astype(int).to_numpy()\n",
    "g_train = df_train[\"person_id\"].to_numpy()\n",
    "\n",
    "X_valid = df_valid[feature_cols]\n",
    "y_valid = df_valid[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "X_test = df_test[feature_cols]\n",
    "y_test = df_test[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "def make_dense_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "prep_tree_dense = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"impute\", SimpleImputer(strategy=\"median\"))]), numeric_cols),\n",
    "        (\"cat\", Pipeline([(\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"onehot\", make_dense_ohe())]), categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def clip01(p, eps=1e-15):\n",
    "    p = np.asarray(p)\n",
    "    return np.clip(p, eps, 1 - eps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70ef8c-0ee3-4733-8ee9-e83f4833ccf6",
   "metadata": {},
   "source": [
    "### Define base model + metrics (version-safe logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a787388-5c35-43ee-8b6d-94a4b3a95af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, brier_score_loss, log_loss\n",
    "\n",
    "def make_base_model():\n",
    "    return Pipeline([\n",
    "        (\"prep\", prep_tree_dense),\n",
    "        (\"clf\", HistGradientBoostingClassifier(\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            max_iter=400,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "def metrics(y_true, p):\n",
    "    y_true = np.asarray(y_true)\n",
    "    p = np.asarray(p)\n",
    "    p_clip = clip01(p)\n",
    "    return {\n",
    "        \"prevalence\": float(y_true.mean()),\n",
    "        \"mean_p\": float(p.mean()),\n",
    "        \"median_p\": float(np.median(p)),\n",
    "        \"pr_auc\": float(average_precision_score(y_true, p)),\n",
    "        \"roc_auc\": float(roc_auc_score(y_true, p)),\n",
    "        \"brier\": float(brier_score_loss(y_true, p)),\n",
    "        \"logloss\": float(log_loss(y_true, p_clip, labels=[0, 1])),\n",
    "    }\n",
    "\n",
    "def ece(y_true, p, n_bins=10):\n",
    "    y_true = np.asarray(y_true)\n",
    "    p = np.asarray(p)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    idx = np.digitize(p, bins) - 1\n",
    "    ece_val = 0.0\n",
    "    n = len(p)\n",
    "    for b in range(n_bins):\n",
    "        m = (idx == b)\n",
    "        if m.sum() == 0:\n",
    "            continue\n",
    "        conf = p[m].mean()\n",
    "        acc = y_true[m].mean()\n",
    "        ece_val += (m.sum() / n) * abs(acc - conf)\n",
    "    return float(ece_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee999013-9bea-400b-8b12-90a112a61924",
   "metadata": {},
   "source": [
    "### Cross-fitted (out-of-fold) predictions on TRAIN (group-safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d19ef40-7aac-4f61-8e5a-4c9c19492d6a",
   "metadata": {},
   "source": [
    "This is the key Day 6 move. We split TRAIN into folds by patient, then for each fold we fit on “other folds” and predict on the held-out fold. That gives us out-of-fold probabilities that are honest for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8223a39-eb07-4cc8-af9c-f00338f79fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5 done. cal size=12198\n",
      "Fold 2/5 done. cal size=12198\n",
      "Fold 3/5 done. cal size=12198\n",
      "Fold 4/5 done. cal size=12197\n",
      "Fold 5/5 done. cal size=12197\n",
      "OOF preds ready. mean(p_oof)= 0.11170352563190443\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "K = 5\n",
    "gkf = GroupKFold(n_splits=K)\n",
    "\n",
    "p_oof = np.zeros(len(df_train), dtype=float)\n",
    "\n",
    "for fold, (tr_idx, cal_idx) in enumerate(gkf.split(X_train, y_train, groups=g_train), start=1):\n",
    "    model = make_base_model()\n",
    "    model.fit(X_train.iloc[tr_idx], y_train[tr_idx])\n",
    "    p_oof[cal_idx] = model.predict_proba(X_train.iloc[cal_idx])[:, 1]\n",
    "    print(f\"Fold {fold}/{K} done. cal size={len(cal_idx)}\")\n",
    "\n",
    "print(\"OOF preds ready. mean(p_oof)=\", float(p_oof.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314bea8b-5f6b-4850-81f1-1f46a5472a94",
   "metadata": {},
   "source": [
    "Sanity check (optional but helpful):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f646e85-0402-410f-9e90-618218b2f4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOF metrics on TRAIN (not for reporting, just sanity): {'prevalence': 0.11218600380402702, 'mean_p': 0.11170352563190443, 'median_p': 0.09638257213695628, 'pr_auc': 0.21460973259443822, 'roc_auc': 0.6670274406569707, 'brier': 0.09528749475684366, 'logloss': 0.33152540670157826}\n",
      "OOF ECE on TRAIN: 0.0011669272167829485\n"
     ]
    }
   ],
   "source": [
    "print(\"OOF metrics on TRAIN (not for reporting, just sanity):\", metrics(y_train, p_oof))\n",
    "print(\"OOF ECE on TRAIN:\", ece(y_train, p_oof))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7130ee6c-e476-4821-9dd4-7564c549032a",
   "metadata": {},
   "source": [
    "### Fit calibrators on OOF predictions (Platt + Isotonic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa7d53f7-7ae3-4e8a-b811-a96bce7a8d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrators fit on OOF predictions.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# Platt (sigmoid) calibration uses logits of probabilities\n",
    "p_oof_c = clip01(p_oof)\n",
    "z_oof = np.log(p_oof_c / (1 - p_oof_c)).reshape(-1, 1)\n",
    "\n",
    "platt = LogisticRegression(solver=\"lbfgs\", C=1e6, max_iter=2000)\n",
    "platt.fit(z_oof, y_train)\n",
    "\n",
    "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "iso.fit(p_oof, y_train)\n",
    "\n",
    "print(\"Calibrators fit on OOF predictions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49f0506-d09a-4e95-a83d-b3840526e4c7",
   "metadata": {},
   "source": [
    "### Fit final base model on full TRAIN, then calibrate VALID and TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd543e9c-f984-4b83-aca3-b2ed795af456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALID base: {'prevalence': 0.11461818181818181, 'mean_p': 0.11277848922639576, 'median_p': 0.09866236945430058, 'pr_auc': 0.22467831773854963, 'roc_auc': 0.6677610282599987, 'brier': 0.09683001463055108, 'logloss': 0.33576765492464755} ECE: 0.004044470923893383\n",
      "VALID sigmoid: {'prevalence': 0.11461818181818181, 'mean_p': 0.11327204954074022, 'median_p': 0.09911347204037121, 'pr_auc': 0.22467831773854963, 'roc_auc': 0.6677610282599987, 'brier': 0.09682699948900617, 'logloss': 0.33575903386252465} ECE: 0.0044861429718006446\n",
      "VALID isotonic: {'prevalence': 0.11461818181818181, 'mean_p': 0.1135638733425948, 'median_p': 0.09396119468061914, 'pr_auc': 0.215795441078798, 'roc_auc': 0.667189460752905, 'brier': 0.09689709281504955, 'logloss': 0.3359497670062253} ECE: 0.0034775109251076308\n",
      "TEST base: {'prevalence': 0.10673348881059892, 'mean_p': 0.1111784920382252, 'median_p': 0.0975009866650095, 'pr_auc': 0.2078325210559714, 'roc_auc': 0.6666140354981995, 'brier': 0.0913437944101995, 'logloss': 0.32108924251898663} ECE: 0.004501189282069633\n",
      "TEST sigmoid: {'prevalence': 0.10673348881059892, 'mean_p': 0.11166645743436707, 'median_p': 0.09794703429186898, 'pr_auc': 0.2078325210559714, 'roc_auc': 0.6666140354981995, 'brier': 0.0913482386427387, 'logloss': 0.3211129468363118} ECE: 0.004971473772442335\n",
      "TEST isotonic: {'prevalence': 0.10673348881059892, 'mean_p': 0.11203489237712684, 'median_p': 0.09396119468061914, 'pr_auc': 0.20102728453515442, 'roc_auc': 0.6663217104189726, 'brier': 0.09134859020495872, 'logloss': 0.3211305982351938} ECE: 0.0056705485053667164\n"
     ]
    }
   ],
   "source": [
    "final_model = make_base_model()\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "p_valid_base = final_model.predict_proba(X_valid)[:, 1]\n",
    "p_test_base  = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Platt transform\n",
    "z_valid = np.log(clip01(p_valid_base) / (1 - clip01(p_valid_base))).reshape(-1, 1)\n",
    "z_test  = np.log(clip01(p_test_base)  / (1 - clip01(p_test_base))).reshape(-1, 1)\n",
    "\n",
    "p_valid_sig = platt.predict_proba(z_valid)[:, 1]\n",
    "p_test_sig  = platt.predict_proba(z_test)[:, 1]\n",
    "\n",
    "# Isotonic transform\n",
    "p_valid_iso = iso.transform(p_valid_base)\n",
    "p_test_iso  = iso.transform(p_test_base)\n",
    "\n",
    "print(\"VALID base:\", metrics(y_valid, p_valid_base), \"ECE:\", ece(y_valid, p_valid_base))\n",
    "print(\"VALID sigmoid:\", metrics(y_valid, p_valid_sig), \"ECE:\", ece(y_valid, p_valid_sig))\n",
    "print(\"VALID isotonic:\", metrics(y_valid, p_valid_iso), \"ECE:\", ece(y_valid, p_valid_iso))\n",
    "\n",
    "print(\"TEST base:\", metrics(y_test, p_test_base), \"ECE:\", ece(y_test, p_test_base))\n",
    "print(\"TEST sigmoid:\", metrics(y_test, p_test_sig), \"ECE:\", ece(y_test, p_test_sig))\n",
    "print(\"TEST isotonic:\", metrics(y_test, p_test_iso), \"ECE:\", ece(y_test, p_test_iso))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebae789e-1b88-4db3-ba50-e30fbad5cbc9",
   "metadata": {},
   "source": [
    "### Pick calibration method using VALID Brier, then do top-K on TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc2fc3ff-1e2e-41d1-a8dd-4498fae303c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best by VALID Brier: sigmoid_platt_oof\n",
      "VALID metrics: {'prevalence': 0.11461818181818181, 'mean_p': 0.11327204954074022, 'median_p': 0.09911347204037121, 'pr_auc': 0.22467831773854963, 'roc_auc': 0.6677610282599987, 'brier': 0.09682699948900617, 'logloss': 0.33575903386252465} ECE: 0.0044861429718006446\n",
      "TEST metrics : {'prevalence': 0.10673348881059892, 'mean_p': 0.11166645743436707, 'median_p': 0.09794703429186898, 'pr_auc': 0.2078325210559714, 'roc_auc': 0.6666140354981995, 'brier': 0.0913482386427387, 'logloss': 0.3211129468363118} ECE: 0.004971473772442335\n",
      "TEST sigmoid_platt_oof {'frac': 0.01, 'k': 201, 'captured': 73, 'precision_at_k': 0.36318407960199006, 'threshold': 0.3494286102081742}\n",
      "TEST sigmoid_platt_oof {'frac': 0.05, 'k': 1007, 'captured': 304, 'precision_at_k': 0.3018867924528302, 'threshold': 0.23179108027082457}\n",
      "TEST sigmoid_platt_oof {'frac': 0.1, 'k': 2015, 'captured': 502, 'precision_at_k': 0.2491315136476427, 'threshold': 0.19038341536822131}\n",
      "TEST sigmoid_platt_oof {'frac': 0.2, 'k': 4030, 'captured': 829, 'precision_at_k': 0.20570719602977666, 'threshold': 0.14351083032576595}\n"
     ]
    }
   ],
   "source": [
    "def topk_summary(y_true, p, frac=0.10):\n",
    "    y_true = np.asarray(y_true)\n",
    "    n = len(y_true)\n",
    "    k = max(1, int(np.floor(frac*n)))\n",
    "    order = np.argsort(-p)\n",
    "    idx = order[:k]\n",
    "    return {\n",
    "        \"frac\": float(frac),\n",
    "        \"k\": int(k),\n",
    "        \"captured\": int(y_true[idx].sum()),\n",
    "        \"precision_at_k\": float(y_true[idx].mean()),\n",
    "        \"threshold\": float(np.quantile(p, 1-frac))\n",
    "    }\n",
    "\n",
    "cand = [\n",
    "    (\"base\", p_valid_base, p_test_base),\n",
    "    (\"sigmoid_platt_oof\", p_valid_sig, p_test_sig),\n",
    "    (\"isotonic_oof\", p_valid_iso, p_test_iso),\n",
    "]\n",
    "\n",
    "# choose by VALID brier\n",
    "best_name, p_valid_best, p_test_best = min(\n",
    "    cand, key=lambda t: metrics(y_valid, t[1])[\"brier\"]\n",
    ")\n",
    "\n",
    "print(\"Best by VALID Brier:\", best_name)\n",
    "print(\"VALID metrics:\", metrics(y_valid, p_valid_best), \"ECE:\", ece(y_valid, p_valid_best))\n",
    "print(\"TEST metrics :\", metrics(y_test,  p_test_best),  \"ECE:\", ece(y_test,  p_test_best))\n",
    "\n",
    "for frac in [0.01, 0.05, 0.10, 0.20]:\n",
    "    print(\"TEST\", best_name, topk_summary(y_test, p_test_best, frac=frac))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994eec4-632b-443b-8b6c-706381293b6c",
   "metadata": {},
   "source": [
    "### Save Day 6 artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc02b447-25f6-4348-9dcf-813eaaf61cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-6\\reports\\DAY06_crossfit_calibration.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "reports_dir = project_root / \"Day-6\" / \"reports\"\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out = {\n",
    "    \"K\": K,\n",
    "    \"method\": \"Cross-fitted calibration: OOF predictions on TRAIN (GroupKFold by person_id), calibrators fit on OOF, applied to VALID/TEST probabilities from final model.\",\n",
    "    \"valid\": {\n",
    "        \"base\": metrics(y_valid, p_valid_base),\n",
    "        \"sigmoid_platt_oof\": metrics(y_valid, p_valid_sig),\n",
    "        \"isotonic_oof\": metrics(y_valid, p_valid_iso),\n",
    "        \"ece\": {\n",
    "            \"base\": ece(y_valid, p_valid_base),\n",
    "            \"sigmoid_platt_oof\": ece(y_valid, p_valid_sig),\n",
    "            \"isotonic_oof\": ece(y_valid, p_valid_iso),\n",
    "        }\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"base\": metrics(y_test, p_test_base),\n",
    "        \"sigmoid_platt_oof\": metrics(y_test, p_test_sig),\n",
    "        \"isotonic_oof\": metrics(y_test, p_test_iso),\n",
    "        \"ece\": {\n",
    "            \"base\": ece(y_test, p_test_base),\n",
    "            \"sigmoid_platt_oof\": ece(y_test, p_test_sig),\n",
    "            \"isotonic_oof\": ece(y_test, p_test_iso),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(reports_dir / \"DAY06_crossfit_calibration.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(out, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", reports_dir / \"DAY06_crossfit_calibration.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2fe55-4f67-42cd-8f98-554a41dc2c2e",
   "metadata": {},
   "source": [
    "Close DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c43af0c3-3256-4fe0-889f-02ce3913e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB closed.\n"
     ]
    }
   ],
   "source": [
    "con.close()\n",
    "print(\"DuckDB closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4341a88-86c6-4e2e-a27f-d3de7d77afc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
