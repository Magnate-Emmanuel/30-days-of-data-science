{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb702c09-f1a4-456c-8be4-a6988faa7a6a",
   "metadata": {},
   "source": [
    "### Connect to DuckDB + load table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2b9621-b486-46fe-9604-fc6eae004b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-1\\data\\warehouse\\day1.duckdb\n",
      "df shape: (101766, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encounter_id</th>\n",
       "      <th>person_id</th>\n",
       "      <th>label</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>num_lab_procedures</th>\n",
       "      <th>num_procedures</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>number_outpatient</th>\n",
       "      <th>number_emergency</th>\n",
       "      <th>number_inpatient</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>change</th>\n",
       "      <th>insulin</th>\n",
       "      <th>A1Cresult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2278392</td>\n",
       "      <td>8222157</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[0-10)</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149190</td>\n",
       "      <td>55629189</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Female</td>\n",
       "      <td>[10-20)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Up</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64410</td>\n",
       "      <td>86047875</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>AfricanAmerican</td>\n",
       "      <td>Female</td>\n",
       "      <td>[20-30)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500364</td>\n",
       "      <td>82442376</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[30-40)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Up</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16680</td>\n",
       "      <td>42519267</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>[40-50)</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Ch</td>\n",
       "      <td>Steady</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   encounter_id  person_id  label  time_in_hospital  num_lab_procedures  \\\n",
       "0       2278392    8222157      0                 1                  41   \n",
       "1        149190   55629189      0                 3                  59   \n",
       "2         64410   86047875      0                 2                  11   \n",
       "3        500364   82442376      0                 2                  44   \n",
       "4         16680   42519267      0                 1                  51   \n",
       "\n",
       "   num_procedures  num_medications  number_outpatient  number_emergency  \\\n",
       "0               0                1                  0                 0   \n",
       "1               0               18                  0                 0   \n",
       "2               5               13                  2                 0   \n",
       "3               1               16                  0                 0   \n",
       "4               0                8                  0                 0   \n",
       "\n",
       "   number_inpatient             race  gender      age admission_type_id  \\\n",
       "0                 0        Caucasian  Female   [0-10)                 6   \n",
       "1                 0        Caucasian  Female  [10-20)                 1   \n",
       "2                 1  AfricanAmerican  Female  [20-30)                 1   \n",
       "3                 0        Caucasian    Male  [30-40)                 1   \n",
       "4                 0        Caucasian    Male  [40-50)                 1   \n",
       "\n",
       "  discharge_disposition_id admission_source_id diabetesMed change insulin  \\\n",
       "0                       25                   1          No     No      No   \n",
       "1                        1                   7         Yes     Ch      Up   \n",
       "2                        1                   7         Yes     No      No   \n",
       "3                        1                   7         Yes     Ch      Up   \n",
       "4                        1                   7         Yes     Ch  Steady   \n",
       "\n",
       "  A1Cresult  \n",
       "0      None  \n",
       "1      None  \n",
       "2      None  \n",
       "3      None  \n",
       "4      None  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "project_root = Path.cwd().resolve()\n",
    "while not (project_root / \"Day-1\").exists():\n",
    "    if project_root == project_root.parent:\n",
    "        raise FileNotFoundError(\"Could not find project root containing Day-1.\")\n",
    "    project_root = project_root.parent\n",
    "\n",
    "db_path = project_root / \"Day-1\" / \"data\" / \"warehouse\" / \"day1.duckdb\"\n",
    "con = duckdb.connect(str(db_path))\n",
    "print(\"Connected to:\", db_path)\n",
    "\n",
    "df = con.execute(\"SELECT * FROM gold_diabetes_features_v1\").df()\n",
    "print(\"df shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf2aae5-84dc-4345-b8cc-ad0030b08dcd",
   "metadata": {},
   "source": [
    "### Train/valid/test split (patient-safe, same 60/20/20 logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e3a87a-6db7-4eba-a84d-27af5f3ccc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 60988 Valid rows: 20625 Test rows: 20153\n",
      "Prevalence train/valid/test: 0.11218600380402702 0.11461818181818181 0.10673348881059892\n",
      "Overlap train-valid: 0\n",
      "Overlap train-test: 0\n",
      "Overlap valid-test: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "y = df[\"label\"].astype(int)\n",
    "groups = df[\"person_id\"]\n",
    "\n",
    "# Hold out TEST (20%)\n",
    "gss1 = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "idx_trainval, idx_test = next(gss1.split(df, y, groups=groups))\n",
    "\n",
    "df_trainval = df.iloc[idx_trainval].copy()\n",
    "df_test = df.iloc[idx_test].copy()\n",
    "\n",
    "# Split TRAIN vs VALID inside trainval (valid is 25% of trainval -> ~20% of total)\n",
    "y_tv = df_trainval[\"label\"].astype(int)\n",
    "g_tv = df_trainval[\"person_id\"]\n",
    "\n",
    "gss2 = GroupShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "idx_train, idx_valid = next(gss2.split(df_trainval, y_tv, groups=g_tv))\n",
    "\n",
    "df_train = df_trainval.iloc[idx_train].copy()\n",
    "df_valid = df_trainval.iloc[idx_valid].copy()\n",
    "\n",
    "print(\"Train rows:\", df_train.shape[0], \"Valid rows:\", df_valid.shape[0], \"Test rows:\", df_test.shape[0])\n",
    "print(\"Prevalence train/valid/test:\",\n",
    "      float(df_train[\"label\"].mean()),\n",
    "      float(df_valid[\"label\"].mean()),\n",
    "      float(df_test[\"label\"].mean()))\n",
    "\n",
    "# overlap checks (must be 0)\n",
    "print(\"Overlap train-valid:\", len(set(df_train[\"person_id\"]) & set(df_valid[\"person_id\"])))\n",
    "print(\"Overlap train-test:\", len(set(df_train[\"person_id\"]) & set(df_test[\"person_id\"])))\n",
    "print(\"Overlap valid-test:\", len(set(df_valid[\"person_id\"]) & set(df_test[\"person_id\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d02da42-acb6-4e5f-9593-6f273dff4686",
   "metadata": {},
   "source": [
    "### Preprocessing (dense one-hot for hist_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15bf3ce2-c852-4f19-92a3-9f8bd5b50f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "id_cols = [\"encounter_id\", \"person_id\", \"label\"]\n",
    "feature_cols = [c for c in df.columns if c not in id_cols]\n",
    "\n",
    "numeric_cols = [\n",
    "    \"time_in_hospital\", \"num_lab_procedures\", \"num_procedures\", \"num_medications\",\n",
    "    \"number_outpatient\", \"number_emergency\", \"number_inpatient\"\n",
    "]\n",
    "categorical_cols = [c for c in feature_cols if c not in numeric_cols]\n",
    "\n",
    "X_train = df_train[feature_cols]\n",
    "y_train = df_train[\"label\"].astype(int)\n",
    "\n",
    "X_valid = df_valid[feature_cols]\n",
    "y_valid = df_valid[\"label\"].astype(int)\n",
    "\n",
    "X_test = df_test[feature_cols]\n",
    "y_test = df_test[\"label\"].astype(int)\n",
    "\n",
    "def make_dense_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "prep_tree_dense = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"impute\", SimpleImputer(strategy=\"median\"))]), numeric_cols),\n",
    "        (\"cat\", Pipeline([(\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"onehot\", make_dense_ohe())]), categorical_cols),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7446852e-adc5-4ff7-a0fe-bfafbfff5de3",
   "metadata": {},
   "source": [
    "### Fit the Day-4 winner (hist_gb) and evaluate baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c90077b2-174c-438c-ab4b-c7d57fc7a0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE (uncalibrated) valid: {'prevalence': 0.11461818181818181, 'mean_p': 0.11277848922639576, 'median_p': 0.09866236945430058, 'pr_auc': 0.22467831773854963, 'roc_auc': 0.6677610282599987, 'brier': 0.09683001463055108, 'logloss': 0.33576765492464755}\n",
      "BASE (uncalibrated) test : {'prevalence': 0.10673348881059892, 'mean_p': 0.1111784920382252, 'median_p': 0.0975009866650095, 'pr_auc': 0.2078325210559714, 'roc_auc': 0.6666140354981995, 'brier': 0.0913437944101995, 'logloss': 0.32108924251898663}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, brier_score_loss, log_loss\n",
    "import numpy as np\n",
    "\n",
    "def metrics(y_true, p):\n",
    "    y_true = np.asarray(y_true)\n",
    "    p = np.asarray(p)\n",
    "\n",
    "    # prevent log(0) by clipping probabilities\n",
    "    p_clip = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    return {\n",
    "        \"prevalence\": float(y_true.mean()),\n",
    "        \"mean_p\": float(p.mean()),\n",
    "        \"median_p\": float(np.median(p)),\n",
    "        \"pr_auc\": float(average_precision_score(y_true, p)),\n",
    "        \"roc_auc\": float(roc_auc_score(y_true, p)),\n",
    "        \"brier\": float(brier_score_loss(y_true, p)),\n",
    "        \"logloss\": float(log_loss(y_true, p_clip, labels=[0, 1])),\n",
    "    }\n",
    "\n",
    "\n",
    "m_valid_base = metrics(y_valid, p_valid)\n",
    "m_test_base  = metrics(y_test, p_test)\n",
    "\n",
    "print(\"BASE (uncalibrated) valid:\", m_valid_base)\n",
    "print(\"BASE (uncalibrated) test :\", m_test_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2892568a-d6c6-4b7a-8c3f-db32fe805a9d",
   "metadata": {},
   "source": [
    "### Calibration (Sigmoid + Isotonic) using VALID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145751b-8550-4a48-b565-180748489a5b",
   "metadata": {},
   "source": [
    "Calibration is “probability fixing”. It usually keeps ranking similar, but improves probability quality (Brier/logloss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a8c9c3d-b973-4e31-8236-62a9fd7a439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIGMOID (Platt) valid: {'prevalence': 0.11461818181818181, 'mean_p': 0.11462417649952247, 'median_p': 0.09924107224593008, 'pr_auc': 0.22467831773854963, 'roc_auc': 0.6677610282599987, 'brier': 0.09681639063245714, 'logloss': 0.3357148386561877}\n",
      "SIGMOID (Platt) test : {'prevalence': 0.10673348881059892, 'mean_p': 0.1129350547535146, 'median_p': 0.09802124987096311, 'pr_auc': 0.2078325210559714, 'roc_auc': 0.6666140354981995, 'brier': 0.09137285830812976, 'logloss': 0.3211598654462384}\n",
      "ISOTONIC valid: {'prevalence': 0.11461818181818181, 'mean_p': 0.11461818181818181, 'median_p': 0.10289236605026078, 'pr_auc': 0.21822358537800177, 'roc_auc': 0.6706074108172614, 'brier': 0.09648336720036713, 'logloss': 0.33432929598511896}\n",
      "ISOTONIC test : {'prevalence': 0.10673348881059892, 'mean_p': 0.1128547497241375, 'median_p': 0.10131332082551595, 'pr_auc': 0.19904850431304646, 'roc_auc': 0.6666061459879116, 'brier': 0.09135924871272744, 'logloss': 0.3259030906318809}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# --- helper: safe clip ---\n",
    "def clip01(p, eps=1e-15):\n",
    "    p = np.asarray(p)\n",
    "    return np.clip(p, eps, 1 - eps)\n",
    "\n",
    "# --- 1) SIGMOID calibration (Platt scaling) on VALID only ---\n",
    "# We calibrate by fitting a logistic regression on the logit of p_valid.\n",
    "p_valid_c = clip01(p_valid)\n",
    "p_test_c  = clip01(p_test)\n",
    "\n",
    "z_valid = np.log(p_valid_c / (1 - p_valid_c)).reshape(-1, 1)\n",
    "z_test  = np.log(p_test_c  / (1 - p_test_c)).reshape(-1, 1)\n",
    "\n",
    "platt = LogisticRegression(solver=\"lbfgs\", C=1e6, max_iter=2000)  # very weak regularization\n",
    "platt.fit(z_valid, y_valid)\n",
    "\n",
    "p_valid_sig = platt.predict_proba(z_valid)[:, 1]\n",
    "p_test_sig  = platt.predict_proba(z_test)[:, 1]\n",
    "\n",
    "# --- 2) ISOTONIC calibration on VALID only ---\n",
    "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "iso.fit(p_valid, y_valid)           # fit mapping p -> y on VALID\n",
    "\n",
    "p_valid_iso = iso.transform(p_valid)\n",
    "p_test_iso  = iso.transform(p_test)\n",
    "\n",
    "# --- evaluate ---\n",
    "m_valid_sig = metrics(y_valid, p_valid_sig)\n",
    "m_test_sig  = metrics(y_test,  p_test_sig)\n",
    "\n",
    "m_valid_iso = metrics(y_valid, p_valid_iso)\n",
    "m_test_iso  = metrics(y_test,  p_test_iso)\n",
    "\n",
    "print(\"SIGMOID (Platt) valid:\", m_valid_sig)\n",
    "print(\"SIGMOID (Platt) test :\", m_test_sig)\n",
    "print(\"ISOTONIC valid:\", m_valid_iso)\n",
    "print(\"ISOTONIC test :\", m_test_iso)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23cddb-d5a6-4836-bfc3-1c6d7562741d",
   "metadata": {},
   "source": [
    "### Expected Calibration Error (ECE) (simple and useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5af34e5-f8b5-44ed-b2da-4f2b6141f47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECE base: 0.004044470923893383\n",
      "ECE sigmoid: 0.0037983936764188473\n",
      "ECE isotonic: 2.1262453077669665e-18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ece(y_true, p, n_bins=10):\n",
    "    y_true = np.asarray(y_true)\n",
    "    p = np.asarray(p)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    idx = np.digitize(p, bins) - 1\n",
    "    ece_val = 0.0\n",
    "    n = len(p)\n",
    "    for b in range(n_bins):\n",
    "        mask = (idx == b)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        conf = p[mask].mean()\n",
    "        acc  = y_true[mask].mean()\n",
    "        ece_val += (mask.sum() / n) * abs(acc - conf)\n",
    "    return float(ece_val)\n",
    "\n",
    "print(\"ECE base:\", ece(y_valid, p_valid))\n",
    "print(\"ECE sigmoid:\", ece(y_valid, p_valid_sig))\n",
    "print(\"ECE isotonic:\", ece(y_valid, p_valid_iso))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb9b9e4-2662-4990-ba92-b2e2112778df",
   "metadata": {},
   "source": [
    "### Choose the calibration method (pick best by VALID Brier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e3de02a-7970-4c2c-a9a1-b0e60ac44e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best calibration by VALID Brier: isotonic\n",
      "VALID metrics: {'prevalence': 0.11461818181818181, 'mean_p': 0.11461818181818181, 'median_p': 0.10289236605026078, 'pr_auc': 0.21822358537800177, 'roc_auc': 0.6706074108172614, 'brier': 0.09648336720036713, 'logloss': 0.33432929598511896}\n",
      "TEST metrics : {'prevalence': 0.10673348881059892, 'mean_p': 0.1128547497241375, 'median_p': 0.10131332082551595, 'pr_auc': 0.19904850431304646, 'roc_auc': 0.6666061459879116, 'brier': 0.09135924871272744, 'logloss': 0.3259030906318809}\n"
     ]
    }
   ],
   "source": [
    "candidates = [\n",
    "    (\"base\", p_valid, p_test, m_valid_base, m_test_base),\n",
    "    (\"sigmoid\", p_valid_sig, p_test_sig, m_valid_sig, m_test_sig),\n",
    "    (\"isotonic\", p_valid_iso, p_test_iso, m_valid_iso, m_test_iso),\n",
    "]\n",
    "\n",
    "best = min(candidates, key=lambda t: t[3][\"brier\"])  # smallest valid Brier\n",
    "best_name, p_valid_best, p_test_best, m_valid_best, m_test_best = best\n",
    "\n",
    "print(\"Best calibration by VALID Brier:\", best_name)\n",
    "print(\"VALID metrics:\", m_valid_best)\n",
    "print(\"TEST metrics :\", m_test_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e18fc-de74-43fa-a3ce-befef03e7b99",
   "metadata": {},
   "source": [
    "### Turn probabilities into an action rule (capacity targeting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb66a5-f398-4b02-ac0a-d3e56c012554",
   "metadata": {},
   "source": [
    "If you can only intervene on, say, the top 10% highest risk, this is the clean operational rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91b67bc4-b99a-49fa-8fa7-5149ff8f902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST {'frac': 0.01, 'k': 201, 'captured': 81, 'precision_at_k': 0.40298507462686567, 'threshold': 0.37264150943396224}\n",
      "TEST {'frac': 0.05, 'k': 1007, 'captured': 309, 'precision_at_k': 0.30685203574975173, 'threshold': 0.23628691983122363}\n",
      "TEST {'frac': 0.1, 'k': 2015, 'captured': 499, 'precision_at_k': 0.24764267990074443, 'threshold': 0.19258295380611581}\n",
      "TEST {'frac': 0.2, 'k': 4030, 'captured': 828, 'precision_at_k': 0.2054590570719603, 'threshold': 0.14360770577933452}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def topk_summary(y_true, p, frac=0.10):\n",
    "    y_true = np.asarray(y_true)\n",
    "    n = len(y_true)\n",
    "    k = max(1, int(np.floor(frac*n)))\n",
    "    order = np.argsort(-p)\n",
    "    idx = order[:k]\n",
    "    return {\n",
    "        \"frac\": frac,\n",
    "        \"k\": k,\n",
    "        \"captured\": int(y_true[idx].sum()),\n",
    "        \"precision_at_k\": float(y_true[idx].mean()),\n",
    "        \"threshold\": float(np.quantile(p, 1-frac))\n",
    "    }\n",
    "\n",
    "for frac in [0.01, 0.05, 0.10, 0.20]:\n",
    "    print(\"TEST\", topk_summary(y_test, p_test_best, frac=frac))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb764941-9f6f-406e-997e-6081838c5438",
   "metadata": {},
   "source": [
    "### Save Day-5 artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38747a17-e3f6-4b2f-a6d3-27f20976f52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-5\\reports\\DAY05_calibration_metrics.json\n",
      "Saved: C:\\Users\\sarfo\\Dropbox\\Courses\\Data Science\\30-days-of-data-science\\Day-5\\reports\\DAY05_top200_test_predictions_best.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "reports_dir = project_root / \"Day-5\" / \"reports\"\n",
    "reports_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "out = {\n",
    "    \"split\": {\"method\": \"GroupShuffleSplit by person_id\", \"approx\": \"60/20/20\", \"random_state\": 42},\n",
    "    \"baseline_uncalibrated\": {\"valid\": m_valid_base, \"test\": m_test_base},\n",
    "    \"sigmoid\": {\"valid\": m_valid_sig, \"test\": m_test_sig},\n",
    "    \"isotonic\": {\"valid\": m_valid_iso, \"test\": m_test_iso},\n",
    "    \"best_by_valid_brier\": {\"name\": best_name, \"valid\": m_valid_best, \"test\": m_test_best},\n",
    "    \"ece_valid\": {\n",
    "        \"base\": ece(y_valid, p_valid),\n",
    "        \"sigmoid\": ece(y_valid, p_valid_sig),\n",
    "        \"isotonic\": ece(y_valid, p_valid_iso),\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(reports_dir / \"DAY05_calibration_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(out, f, indent=2)\n",
    "\n",
    "# Save top-200 predictions on TEST using best probabilities\n",
    "top = df_test[[\"encounter_id\", \"person_id\", \"label\"]].copy()\n",
    "top[\"p_hat\"] = p_test_best\n",
    "top = top.sort_values(\"p_hat\", ascending=False).head(200)\n",
    "top.to_csv(reports_dir / \"DAY05_top200_test_predictions_best.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\", reports_dir / \"DAY05_calibration_metrics.json\")\n",
    "print(\"Saved:\", reports_dir / \"DAY05_top200_test_predictions_best.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a94390-a655-44ea-9af1-9cd92677edfc",
   "metadata": {},
   "source": [
    "Close DuckDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1b7c37d-6117-461e-9541-74522033612e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB closed.\n"
     ]
    }
   ],
   "source": [
    "con.close()\n",
    "print(\"DuckDB closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7548f8-b666-4b9f-be4f-4ca8ab300567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
